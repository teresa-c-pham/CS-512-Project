{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Wh9nKdk-cQJo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/user/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/user/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/user/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/user/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/user/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /Users/user/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/user/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-macosx_10_11_x86_64.whl (4.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /Users/user/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/user/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/user/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/user/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/user/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/user/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/user/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-4WmEy21dQir"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wk/st02spn17dgdzhg1fswg_v740000gn/T/ipykernel_92542/2936876116.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sh90SGLQdUhw"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels=1\n",
    "    )\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "V1AX7fC8dYau"
   },
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "with open('original_rt_snippets.txt', 'r') as f:\n",
    "    data_texts = f.readlines()\n",
    "\n",
    "with open('sentiment_labels.txt', 'r') as f:\n",
    "    data_label = f.readlines()\n",
    "\n",
    "data_texts = data_texts[0:100]\n",
    "data_label = data_label[1:101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CmQt41wyfsf0"
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "sentences = []\n",
    "\n",
    "for i in range(len(data_texts)):\n",
    "  label_id, label_val = data_label[i].split('|')\n",
    "  labels.append(float(label_val[:-1]))\n",
    "  sentences.append(data_texts[i][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "0XodnJDyZCP6"
   },
   "outputs": [],
   "source": [
    "# Tokenize input sentences\n",
    "encoded_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels_tensor = torch.FloatTensor(labels).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09NP1k8h-3JG"
   },
   "outputs": [],
   "source": [
    "def remover_str(my_string, value3):\n",
    "    for item in my_string:\n",
    "      if item not in values3:\n",
    "        my_string = my_string.replace(item, \"\")\n",
    "    return my_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFDY5Sip-hxp"
   },
   "outputs": [],
   "source": [
    "# Methodology to assign label when no label given\n",
    "# The map Reduce will base on the sentiment to assign\n",
    "# an appropriate score the a sentence\n",
    "def map_reduce(dataFrame):\n",
    "  if len(dataFrame) == 0:\n",
    "    return None\n",
    "  \n",
    "  sentimentString = dataFrame.astype(str)\n",
    "  tDesc = sentimentString['description']        # has all the original tweets\n",
    "\n",
    "  values3 = list(\"abcdefghijklmnopqrstuvwxyz\")  # Original list of alphabet\n",
    "\n",
    "  sentimentString = dataFrame.astype(str)\n",
    "  desWords = sentimentString['description'].apply(str.lower).str.split()\n",
    "\n",
    "  n = 0;  #assigns a unique n value to each sentence\n",
    "\n",
    "  reviewSentiment = {} # used to save n as key and score as value\n",
    "  descLen = {}         # saves n as key and score/length as value\n",
    "\n",
    "  for string in desWords:\n",
    "    length = 0\n",
    "    reviewSentiment[n] = 0\n",
    "    for word in string:\n",
    "      length += 1\n",
    "      word = remover_str(word, value3)\n",
    "      if word in positive:\n",
    "        reviewSentiment[n] += 1\n",
    "      if word in negative:\n",
    "        reviewSentiment[n] -= 1\n",
    "\n",
    "    if reviewSentiment[n] != 0:\n",
    "      descLen[n] = reviewSentiment[n]/length\n",
    "    n += 1\n",
    "\n",
    "  return reviewSentiment, descLen, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-Kc5Yw1rgv4j"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**encoded_inputs, labels=labels_tensor)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch:', epoch, 'Training Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NMcJsfmNg-cq"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "    predictions = outputs.logits.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "O-BXf7vrhAiC"
   },
   "outputs": [],
   "source": [
    "# Print some example predictions\n",
    "for i in range(10):\n",
    "    print(\"Sentence:\", sentences[i])\n",
    "    print(\"True score:\", labels[i])\n",
    "    print(\"Predicted score:\", predictions[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "bWOUGVbwk301"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Prediction score graph visualization\n",
    "predict_score = predictions\n",
    "texting_label = range(0, len(predict_score))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(texting_label, predict_score, marker='')\n",
    "plt.xlabel('Text Label Contents ID')\n",
    "plt.ylabel('Text Label Scores w/ID')\n",
    "plt.title('Plot Prediction score graph visualization using Transformer-Based Model')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Ky1vfvzrnM7A"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(texting_label, labels, marker='')\n",
    "plt.xlabel('Text Label Contents ID')\n",
    "plt.ylabel('Text Label True Scores')\n",
    "plt.title('Plot True Scores graph visualization using Transformer-Based Model')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "AFpOIq2Nt9ZI"
   },
   "outputs": [],
   "source": [
    "text_string = \"Hello, I kinda love this movie and I may join that again in the future\"\n",
    "\n",
    "# Tokenize input string\n",
    "encoded_input = tokenizer(text_string, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Pass input string through the model to obtain predicted score\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input)\n",
    "    predicted_score = outputs.logits.item()\n",
    "\n",
    "print(\"Predicted score for the input text is:\", predicted_score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
